Design for Data Degrowth is a method of design for digital interfaces, tools, and experiences, in which the use of a product discourages the production of (more) data.

---
## On Data Degrowth

In the technological sector, there is a shared belief that every problem can virtually be solved, given an infinite amount of computational power, data, or raw material. The increasing reliance on these finite resources, though, now poses the gravest problem itself. The belief is becoming a paradox: growing (more) data is no longer sustainable. It never was. But the myth of data growth as the only possible solution makes this practice seem irreplaceable.

Decades of science reports and climate disasters begged us to look at the damage the planet suffered from the extractive practices of industrial growth. In the meantime, political ecologists and climate activists are formulating new societal models countering growth. Degrowth represents maybe the most encompassing school of thought: a political, economic, and social movement on a mission to reduce global consumption and production. Not only to stop an otherwise unsustainable economic practice, but, too, as the required premise to social and ecological justice.

Degrowth, simply put, rejects a culture based on overconsumption and imagines a society beyond GDP. In this context, Degrowth asks us, consumers, to rethink our own habits and behaviours. The new cultural framework is a proposal of enoughness: one that finds wellbeing, freedom, and beauty in living with limits.

Data Degrowth is the rehearsal of this framework in our digital realm. For a renewed set of values where less data is more. Where storage limits are approached with care, and computational resources with fear and respect. In short, Data Degrowth is a proposition to strengthen our understanding of the relationship between planetary resources and our own data habits.

---
## Principles of Design for Data Degrowth

Data Degrowth questions the designs through which we interface with data, and translates principles of Degrowth into a new framework of design thinking. Needless to say, design historically deals with concepts of "constraints" and "efficiency". Here, these concepts are revisited through a political and ecological lens, with a narrow focus on data interfaces. Listed below, are some common denominators of Data Degrowth design.
### Design with Enoughness

Enoughness can be described as the experience of inherent contentment, as opposed to a culture of "moreness". In fact, "more" of a resource does not directly equate to a better result. 

In the wilderness, vegetation is "designed" to thrive with just enough resources. A right amount of water makes a plant grow strong, whereas too much rain results in floods and destruction. Then again, a plant might need different intakes of water at different phases of its life, or throughout different seasons, species, ecosystems. Enoughness is not a standard value, but rather a locally situated variable, balancing satisfaction and tolerance.

The *automatic faucet* is an example of design that carries this principle: a tap equipped with a proximity sensor and a mechanism that opens its valve *only* in presence of the user's hands. Easy to spot in a public hospital, the *automatic facet* provides *enough* water at the user's will, reducing water waste and the spread of microbes. In design terms, the sink *affords* the rinsing of hands, so the resources used by the *automatic faucet* are never greater than the tool's affordance. In simple terms: you get water *only as long as* you put your hands under the sink, so that all the water used goes into rinsing your hands.

In Data Degrowth, enoughness translates to designs that water *just enough*. Where the data produced, and their impact, is minimal to the necessary amount, satisfying an affordance like storing, moving, or producing data, without usage overkill.

Large Language Model *GPT3*, released by OpenAI and famously deployed through the chatbot ChatGPT, trains on a dataset of 570GB and 17.5 trillions (!!) of parameters. Aside from issues of data transparency, participation, and moderation of such a big dataset, the sheer size of this model makes it inaccessible to those interested in deploying it locally, on a repurposed device, or in other conditions of limited data storage and computing. The counter-proposals is one of Small Language Models, or models using datasets *as large as can be sufficiently documented*. The idea of sufficiency responds to the question of enoughness. When is a language model good enough? When is it exactly fit for its purpose, ergo when does it not need any more resources?

Human scale language models! The idea of creating a linear relationship between the documentation of a dataset and its size discourages a developer from causing gigantic collections of data. The *automatic faucet* of Language Models, then, is one that provides data to the model only whereas the user needs it. The usage of data equals the affordance of a generative text tool, and spares any unnecessary extra resources.
### Design with Finitude

### Design with Conviviality

### Design with Locality

### Design with Temporality
