*Deep learning's recent history has been one of achievement: from triumphing over humans in the game of Go to world-leading performance in image classification, voice recognition, translation, and other tasks. But this progress has come with a voracious appetite for computing power. This article catalogues the extent of this dependency, showing that progress across a wide variety of applications is strongly reliant on increases in computing power. Extrapolating forward this reliance reveals that progress along current lines is rapidly becoming economically, technically, and environmentally unsustainable. Thus, continued progress in these applications will require dramatically more computationally-efficient methods, which will either have to come from changes to deep learning or from moving to other machine learning methods.*

The disruptive achievements of Deep Learning in the past decade have come with a great ecological price. In fact, the progress of ML models for image classification, natural language processing, and voice recognition has been reliant on increasing computing power. The computational burden of AI is economically, technically, and environmentally unsustainable.

Continued progress in these applications will require alternative approaches, or dramatically more computationally-efficient methods, which will either come from changes to deep learning or from moving to other machine learning methods.

what is pruning?

what is inference?

how does a language model work? Why is it bad for the environment? What are the alternatives? Why are they not as easy to use? What is vendor lock-in? What is digital enclosure?

The inquiry here is not even about rejecting a technology, but rather critiquing the hierarchical state of power that controls and deploys it. Every technology belongs to the people!